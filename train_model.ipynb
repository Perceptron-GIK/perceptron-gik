{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GIK Character Prediction Model\n",
        "\n",
        "Train a model to predict keyboard characters from IMU sensor data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add project root to path\n",
        "PROJECT_ROOT = os.path.dirname(os.path.abspath('__file__'))\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "from pretraining import preprocess_multiple_sources, load_preprocessed_dataset, export_dataset_to_csv\n",
        "from src.pre_processing.alignment import INDEX_TO_CHAR, CHAR_TO_INDEX, NUM_CLASSES\n",
        "from src.pre_processing.reduce_dim import reduce_dim\n",
        "from ml.models.gik_model import create_model_auto_input_dim, GIKTrainer, decode_predictions\n",
        "from ml.models.loss_functions.custom_losses import FocalLoss, CoordinateLoss\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "print(f\"PyTorch {torch.__version__} | Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data paths - Using data_hazel_2 with multiple sources\n",
        "DATA_DIR = \"data_right_all\"\n",
        "\n",
        "# Multiple data sources (paired keyboard + IMU files)\n",
        "KEYBOARD_FILES = ['Keyboard_4.csv', 'Keyboard_5.csv', 'Keyboard_6.csv', 'Keyboard_7.csv', 'Keyboard_8.csv', 'Keyboard_9.csv', 'Keyboard_11.csv']\n",
        "LEFT_FILES = None\n",
        "RIGHT_FILES = ['Right_4.csv', 'Right_5.csv', 'Right_6.csv', 'Right_7.csv', 'Right_8.csv', 'Right_9.csv', 'Right_11.csv']  # Set to list of files if available\n",
        "\n",
        "PROCESSED_DATA_PATH = os.path.join(DATA_DIR, \"processed_dataset.pt\")\n",
        "\n",
        "REDUCE_DIM = True # Whether to apply dimensionality reduction\n",
        "\n",
        "# Training config - OPTIMIZED (best: LSTM + Focal Loss ~37% test acc)\n",
        "CONFIG = {\n",
        "    # PRE-PROCESSING ARGS\n",
        "    'max_seq_length': 10,\n",
        "    \n",
        "    # GIK Model WRAPPER ARGS (Input Projection Layer + Classification Head) \n",
        "    'hidden_dim_classification_head': 64,\n",
        "    'num_layers': 1,\n",
        "    'dropout': 0.3,  # Increased for regularization\n",
        "    \n",
        "    # Dataset Args\n",
        "    'is_one_hot': True,\n",
        "    'key_mapping_dict': CHAR_TO_INDEX,\n",
        "    \n",
        "    # TRAINER ARGS \n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 5e-4,\n",
        "    'weight_decay': 1e-3,  # L2 regularization\n",
        "    'epochs': 200,\n",
        "    'early_stopping': 30,\n",
        "    \n",
        "    # INNER MODEL ARGS\n",
        "    # Model: 'lstm' (best) | 'transformer' | 'attention_lstm' | 'gru' | 'rnn' | 'cnn'\n",
        "    'model_type': 'lstm',\n",
        "    'hidden_dim_inner_model': 128,\n",
        "    'inner_model_prams': {'bidirectional' : True, 'dropout' : 0.5, 'num_layers' : 2},\n",
        "    \n",
        "    # LOSS Function ARGS\n",
        "    'loss': FocalLoss,  # Handles class imbalance\n",
        "    'loss_params': {\"gamma\" : 2.0},\n",
        "}\n",
        "\n",
        "print(f\"Data dir: {DATA_DIR}\")\n",
        "print(f\"Keyboard files: {KEYBOARD_FILES}\")\n",
        "print(f\"Left IMU files: {LEFT_FILES}\")\n",
        "print(f\"Right IMU files: {RIGHT_FILES}\")\n",
        "print(f\"Model: {CONFIG['model_type']}\")\n",
        "print(f\"Seq length: {CONFIG['max_seq_length']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess and combine multiple data sources\n",
        "metadata = preprocess_multiple_sources(\n",
        "    data_dir=DATA_DIR,\n",
        "    keyboard_files=KEYBOARD_FILES,\n",
        "    left_files=LEFT_FILES,\n",
        "    right_files=RIGHT_FILES,\n",
        "    output_path=PROCESSED_DATA_PATH,\n",
        "    max_seq_length=CONFIG['max_seq_length'],\n",
        "    normalize=True,\n",
        "    apply_filtering=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nTotal Samples: {metadata['num_samples']} | Feat dim: {metadata['feat_dim']} | Sources: {metadata['num_sources']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export to CSV for inspection (optional)\n",
        "export_dataset_to_csv(PROCESSED_DATA_PATH, DATA_DIR, include_features=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature dimension reduced from 59 to 23\n"
          ]
        }
      ],
      "source": [
        "if REDUCE_DIM:\n",
        "    DIM_RED_OUTPUT = os.path.join(DATA_DIR, \"dim_red_output.pt\")\n",
        "\n",
        "    # Ideally pass the metadata from preprocessing instead of manually setting it here but this avoids having to rerun the preprocessing step\n",
        "    HAS_LEFT = False\n",
        "    HAS_RIGHT = True\n",
        "\n",
        "    # dims = reduce_dim(\n",
        "    #     data_dir=PROCESSED_DATA_PATH,\n",
        "    #     method=\"active-imu\",\n",
        "    #     has_left=HAS_LEFT,\n",
        "    #     has_right=HAS_RIGHT, \n",
        "    #     normalise=True,\n",
        "    #     output_path=DIM_RED_OUTPUT)\n",
        "    \n",
        "    dims = reduce_dim(\n",
        "        data_dir=PROCESSED_DATA_PATH,\n",
        "        method=\"pca\",\n",
        "        dims_ratio=0.4,\n",
        "        has_left=HAS_LEFT,\n",
        "        has_right=HAS_RIGHT, \n",
        "        normalise=True,\n",
        "        output_path=DIM_RED_OUTPUT)\n",
        "\n",
        "    print(f\"Feature dimension reduced from {dims['dim_bef']} to {dims['dim_aft']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset & Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = load_preprocessed_dataset(DIM_RED_OUTPUT if REDUCE_DIM else PROCESSED_DATA_PATH, \n",
        "                                    is_one_hot_labels=CONFIG[\"is_one_hot\"],\n",
        "                                    char_to_index=CONFIG[\"key_mapping_dict\"])\n",
        "print(f\"Dataset: {len(dataset)} samples | Input dim: {dataset.input_dim}\")\n",
        "\n",
        "# Create model\n",
        "model = create_model_auto_input_dim(\n",
        "    dataset,\n",
        "    model_type=CONFIG['model_type'],\n",
        "    hidden_dim_inner_model=CONFIG['hidden_dim_inner_model'],\n",
        "    hidden_dim_classification_head=CONFIG['hidden_dim_classification_head'],\n",
        "    no_layers_classification_head=CONFIG['num_layers'],\n",
        "    dropout_inner_layers=CONFIG['dropout'],\n",
        "    inner_model_kwargs=CONFIG['inner_model_prams']\n",
        ")\n",
        "\n",
        "# Print model architecture\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Model architecture\")\n",
        "print(\"=\" * 60)\n",
        "print(model)\n",
        "print(\"=\" * 60)\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = GIKTrainer(\n",
        "    model=model,\n",
        "    dataset=dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    learning_rate=CONFIG['learning_rate'],\n",
        "    device=DEVICE,\n",
        "    loss=CONFIG.get('loss'),\n",
        "    loss_kwargs=CONFIG.get('loss_params')\n",
        ")\n",
        "\n",
        "history = trainer.train(\n",
        "    epochs=CONFIG['epochs'],\n",
        "    early_stopping_patience=CONFIG['early_stopping']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].plot(history['train_loss'], label='Train')\n",
        "axes[0].plot(history['val_loss'], label='Val')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "axes[0].set_title('Loss')\n",
        "\n",
        "axes[1].plot(history['train_acc'], label='Train')\n",
        "axes[1].plot(history['val_acc'], label='Val')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].set_title('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 2.2458 | Validation Accuracy: 30.77%\n",
            "\n",
            "Test Loss: 2.3130 | Test Accuracy: 31.73%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on validation set\n",
        "val_loss, val_acc = trainer.validate()\n",
        "print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_acc:.2%}\")\n",
        "print()\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = trainer.evaluate_test()\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to display predictions\n",
        "def show_predictions(data_subset, subset_name, max_samples=50):\n",
        "    \"\"\"Show predictions for a dataset subset.\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    results = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx in range(len(data_subset)):\n",
        "            x, y_true = data_subset[idx]\n",
        "            x = x.unsqueeze(0).to(DEVICE)\n",
        "            y_pred = model(x)\n",
        "            \n",
        "            true_char = INDEX_TO_CHAR.get(y_true.argmax().item(), '?')\n",
        "            pred_char = INDEX_TO_CHAR.get(y_pred.argmax().item(), '?')\n",
        "            \n",
        "            is_correct = true_char == pred_char\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "            \n",
        "            results.append((true_char, pred_char, is_correct))\n",
        "    \n",
        "    # Print header\n",
        "    print(f\"=== {subset_name} Predictions ({correct}/{len(results)} correct = {correct/len(results):.2%}) ===\")\n",
        "    print()\n",
        "    \n",
        "    # Show samples (up to max_samples)\n",
        "    n_show = min(max_samples, len(results))\n",
        "    for i, (true_char, pred_char, is_correct) in enumerate(results[:n_show]):\n",
        "        # Display special chars nicely\n",
        "        true_disp = {'\\n': 'ENTER', ' ': 'SPACE', '\\t': 'TAB', '\\b': 'BKSP'}.get(true_char, true_char)\n",
        "        pred_disp = {'\\n': 'ENTER', ' ': 'SPACE', '\\t': 'TAB', '\\b': 'BKSP'}.get(pred_char, pred_char)\n",
        "        match = '✓' if is_correct else '✗'\n",
        "        print(f\"{match} True: {true_disp:8} | Pred: {pred_disp:8}\")\n",
        "    \n",
        "    if len(results) > n_show:\n",
        "        print(f\"... and {len(results) - n_show} more samples\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show validation predictions\n",
        "show_predictions(trainer.val_dataset, 'Validation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show test predictions\n",
        "show_predictions(trainer.test_dataset, 'Test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "# Refined coordinates for letters and digits, trying to map out the real QWERTY keyboard layout\n",
        "KEY_COORDS = {\n",
        "\n",
        "    '1': (0.0, 0.0), '2': (1.0, 0.0), '3': (2.0, 0.0), '4': (3.0, 0.0), '5': (4.0, 0.0), '6': (5.0, 0.0), '7': (6.0, 0.0), '8': (7.0, 0.0), '9': (8.0, 0.0), '0': (9.0, 0.0),\n",
        "\n",
        "    'q': (0.5, 1.0), 'w': (1.5, 1.0), 'e': (2.5, 1.0), 'r': (3.5, 1.0), 't': (4.5, 1.0), 'y': (5.5, 1.0), 'u': (6.5, 1.0), 'i': (7.5, 1.0), 'o': (8.5, 1.0), 'p': (9.5, 1.0),\n",
        "\n",
        "    'a': (0.8, 2.0), 's': (1.8, 2.0), 'd': (2.8, 2.0), 'f': (3.8, 2.0), 'g': (4.8, 2.0), 'h': (5.8, 2.0), 'j': (6.8, 2.0), 'k': (7.8, 2.0), 'l': (8.8, 2.0),\n",
        "\n",
        "    'z': (1.3, 3.0), 'x': (2.3, 3.0), 'c': (3.3, 3.0), 'v': (4.3, 3.0), 'b': (5.3, 3.0), 'n': (6.3, 3.0), 'm': (7.3, 3.0),\n",
        "}\n",
        "\n",
        "# Approximate coordinates for specials so they sit in plausible places\n",
        "SPECIAL_COORDS = {\n",
        "    '\\n': (12.0, 2.0),  # enter to the right of L\n",
        "    '\\b': (12.0, 0.0),  # backspace to the right of P\n",
        "    '\\t': (-0.5, 1.0),  # tab to the left of Q\n",
        "}\n",
        "\n",
        "SPACE_ANCHORS = [ # Make space bar long\n",
        "    (3.3, 4.0),  # under C\n",
        "    (4.3, 4.0),  # under v\n",
        "    (5.3, 4.0),  # under B\n",
        "    (6.3, 4.0),  # under N\n",
        "    (7.3, 4.0),  # under M\n",
        "\n",
        "]\n",
        "\n",
        "ALL_CHARS = [INDEX_TO_CHAR[i] for i in range(40)]\n",
        "\n",
        "FULL_COORDS = {}\n",
        "for ch in ALL_CHARS:\n",
        "    if ch in KEY_COORDS:\n",
        "        FULL_COORDS[ch] = KEY_COORDS[ch]\n",
        "    elif ch in SPECIAL_COORDS:\n",
        "        FULL_COORDS[ch] = SPECIAL_COORDS[ch]\n",
        "    elif ch == ' ':\n",
        "        # handledvia SPACE_ANCHORS below\n",
        "        continue\n",
        "    else:\n",
        "        # generic fallback (e.g., if something unexpected appears)\n",
        "        FULL_COORDS[ch] = (5.3, 4.0) # default to spacebar center\n",
        "\n",
        "\n",
        "def key_distance(ch1, ch2):\n",
        "    \"\"\"\n",
        "    Distance between two keys.\n",
        "    For space (' '), use the minimum distance to its multiple anchor points\n",
        "    to mimic a long bar; other keys use their single coordinate.\n",
        "    \"\"\"\n",
        "    def coords(ch):\n",
        "        if ch == ' ':\n",
        "            return SPACE_ANCHORS\n",
        "        else:\n",
        "            return [FULL_COORDS[ch]]\n",
        "\n",
        "    pts1 = coords(ch1)\n",
        "    pts2 = coords(ch2)\n",
        "\n",
        "    d_min = float('inf')\n",
        "    for (x1, y1) in pts1:\n",
        "        for (x2, y2) in pts2:\n",
        "            d = ((x1 - x2)**2 + (y1 - y2)**2)**0.5\n",
        "            if d < d_min:\n",
        "                d_min = d\n",
        "    return d_min\n",
        "\n",
        "\n",
        "def make_display_label(c):\n",
        "    if c == ' ':\n",
        "        return 'SP'\n",
        "    elif c == '\\n':\n",
        "        return 'ENT'\n",
        "    elif c == '\\b':\n",
        "        return 'BS'\n",
        "    elif c == '\\t':\n",
        "        return 'TAB'\n",
        "    else:\n",
        "        return c\n",
        "\n",
        "\n",
        "def compute_confusion_matrix_40x40(data_subset, model, device):\n",
        "    \"\"\"Return 40x40 confusion matrix in original index order.\"\"\"\n",
        "    model.eval()\n",
        "    y_true_list = []\n",
        "    y_pred_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in range(len(data_subset)):\n",
        "            x, y_true = data_subset[idx]\n",
        "            x = x.unsqueeze(0).to(device)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            y_true_list.append(y_true.argmax().item())\n",
        "            y_pred_list.append(y_pred.argmax().item())\n",
        "\n",
        "    labels_idx = list(range(40))\n",
        "    cm_orig = confusion_matrix(y_true_list, y_pred_list, labels=labels_idx)\n",
        "\n",
        "    \n",
        "    return cm_orig\n",
        "\n",
        "def plot_confusion_matrix_40x40(cm_orig, subset_name):\n",
        "    \"\"\"Plot the full 40x40 confusion matrix in original index order.\"\"\"\n",
        "    display_labels = [make_display_label(INDEX_TO_CHAR[i]) for i in range(40)]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(16, 14))\n",
        "    sns.heatmap(\n",
        "        cm_orig,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=display_labels,\n",
        "        yticklabels=display_labels,\n",
        "        ax=ax,\n",
        "        annot_kws={'size': 7}\n",
        "    )\n",
        "\n",
        "    ax.xaxis.set_label_position('top')\n",
        "    ax.xaxis.tick_top()\n",
        "    ax.set_xlabel('Predicted', fontsize=12)\n",
        "    ax.set_ylabel('True', fontsize=12)\n",
        "    ax.set_title(\n",
        "        f'{subset_name} Confusion Matrix (40 classes, raw counts)',\n",
        "        fontsize=14,\n",
        "        pad=40\n",
        "    )\n",
        "\n",
        "    plt.xticks(rotation=45, ha='right', fontsize=8)\n",
        "    plt.yticks(rotation=0, fontsize=8)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4. Local distance‑ordered view: anchor + 6 closest neighbours\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def plot_anchor_with_closest_neighbours(cm_orig, anchor_char, subset_name, k_neighbours=6):\n",
        "    \"\"\"\n",
        "    For a given anchor_char, find its k_neighbours closest keys (by keyboard distance),\n",
        "    and plot a confusion submatrix over [anchor + neighbours] in distance order\n",
        "    (anchor first, then increasing distance). Also return that neighbour list.\n",
        "    \"\"\"\n",
        "    if anchor_char not in FULL_COORDS:\n",
        "        raise ValueError(f'No coords for anchor_char {anchor_char!r}')\n",
        "\n",
        "    # char <-> index\n",
        "    char_to_idx = {ch: idx for idx, ch in INDEX_TO_CHAR.items()}\n",
        "\n",
        "    # sort all other chars by distance from anchor\n",
        "    others = [ch for ch in ALL_CHARS if ch != anchor_char]\n",
        "    others_sorted = sorted(others, key=lambda ch: key_distance(anchor_char, ch))\n",
        "\n",
        "    # take closest k_neighbours\n",
        "    closest_chars = others_sorted[:k_neighbours]\n",
        "\n",
        "    # final ordered list: anchor first, then neighbours\n",
        "    ordered_chars = [anchor_char] + closest_chars\n",
        "    ordered_indices = [char_to_idx[ch] for ch in ordered_chars]\n",
        "    ordered_display = [make_display_label(ch) for ch in ordered_chars]\n",
        "\n",
        "    # submatrix of cm_orig\n",
        "    cm_sub = cm_orig[ordered_indices][:, ordered_indices]\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    sns.heatmap(\n",
        "        cm_sub,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=ordered_display,\n",
        "        yticklabels=ordered_display,\n",
        "        ax=ax,\n",
        "        annot_kws={'size': 10}\n",
        "    )\n",
        "\n",
        "    ax.xaxis.set_label_position('top')\n",
        "    ax.xaxis.tick_top()\n",
        "    ax.set_xlabel('Predicted', fontsize=12)\n",
        "    ax.set_ylabel('True', fontsize=12)\n",
        "    ax.set_title(\n",
        "        f'{subset_name}: \"{anchor_char}\" + {k_neighbours} closest keys',\n",
        "        fontsize=14,\n",
        "        pad=40\n",
        "    )\n",
        "\n",
        "    plt.xticks(rotation=45, ha='right', fontsize=9)\n",
        "    plt.yticks(rotation=0, fontsize=9)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return closest_chars, cm_sub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Keyboard Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "def plot_virtual_keyboard_heatmap(cm_orig, anchor_char, subset_name):\n",
        "\n",
        "    char_to_idx = {ch: idx for idx, ch in INDEX_TO_CHAR.items()}\n",
        "    if anchor_char not in char_to_idx:\n",
        "        raise ValueError(f'Unknown anchor_char {anchor_char!r}')\n",
        "\n",
        "    i_anchor = char_to_idx[anchor_char]\n",
        "    row = cm_orig[i_anchor].astype(float)  # length 40\n",
        "\n",
        "    values = {INDEX_TO_CHAR[i]: row[i] for i in range(40)}\n",
        "\n",
        "    all_vals = np.array(list(values.values()))\n",
        "    vmax = all_vals.max() if np.any(all_vals > 0) else 1.0\n",
        "    norm = mcolors.Normalize(vmin=0, vmax=vmax)\n",
        "    cmap = plt.cm.Reds\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "    key_width = 0.8\n",
        "    key_height = 0.8\n",
        "\n",
        "    for ch, (x, y) in KEY_COORDS.items():\n",
        "        v = values.get(ch, 0.0)\n",
        "        color = cmap(norm(v)) if v > 0 else (0.9, 0.9, 0.9, 1.0)\n",
        "        rect = plt.Rectangle(\n",
        "            (x - key_width/2, y - key_height/2),\n",
        "            key_width,\n",
        "            key_height,\n",
        "            edgecolor='k',\n",
        "            facecolor=color,\n",
        "            zorder=2\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x, y, ch.upper(), ha='center', va='center', fontsize=10, zorder=3)\n",
        "\n",
        "    for ch, (x, y) in SPECIAL_COORDS.items():\n",
        "        if ch == ' ':\n",
        "            continue\n",
        "        v = values.get(ch, 0.0)\n",
        "        color = cmap(norm(v)) if v > 0 else (0.9, 0.9, 0.9, 1.0)\n",
        "        label = {'\\n': 'ENT', '\\b': 'BS', '\\t': 'TAB'}.get(ch, ch)\n",
        "        rect = plt.Rectangle(\n",
        "            (x - key_width/2, y - key_height/2),\n",
        "            key_width,\n",
        "            key_height,\n",
        "            edgecolor='k',\n",
        "            facecolor=color,\n",
        "            zorder=2\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x, y, label, ha='center', va='center', fontsize=9, zorder=3)\n",
        "\n",
        "    xs = [p[0] for p in SPACE_ANCHORS]\n",
        "    y_space = SPACE_ANCHORS[0][1]\n",
        "    x_min, x_max = min(xs) - 0.4, max(xs) + 0.4\n",
        "    space_width = x_max - x_min\n",
        "    space_height = 0.8\n",
        "\n",
        "    v_space = values.get(' ', 0.0)\n",
        "    color_space = cmap(norm(v_space)) if v_space > 0 else (0.9, 0.9, 0.9, 1.0)\n",
        "\n",
        "    space_rect = plt.Rectangle(\n",
        "        (x_min, y_space - space_height/2),\n",
        "        space_width,\n",
        "        space_height,\n",
        "        edgecolor='k',\n",
        "        facecolor=color_space,\n",
        "        zorder=1\n",
        "    )\n",
        "    ax.add_patch(space_rect)\n",
        "    ax.text((x_min + x_max) / 2, y_space, 'SP', ha='center', va='center', fontsize=10, zorder=3)\n",
        "\n",
        "    # Highlight anchor with thick border\n",
        "    def outline_anchor(ch, lw=2.5, color='blue'):\n",
        "        if ch == ' ':\n",
        "            outline = plt.Rectangle(\n",
        "                (x_min, y_space - space_height/2),\n",
        "                space_width,\n",
        "                space_height,\n",
        "                fill=False,\n",
        "                edgecolor=color,\n",
        "                linewidth=lw,\n",
        "                zorder=4\n",
        "            )\n",
        "            ax.add_patch(outline)\n",
        "        elif ch in KEY_COORDS:\n",
        "            x, y = KEY_COORDS[ch]\n",
        "            outline = plt.Rectangle(\n",
        "                (x - key_width/2, y - key_height/2),\n",
        "                key_width,\n",
        "                key_height,\n",
        "                fill=False,\n",
        "                edgecolor=color,\n",
        "                linewidth=lw,\n",
        "                zorder=4\n",
        "            )\n",
        "            ax.add_patch(outline)\n",
        "        elif ch in SPECIAL_COORDS:\n",
        "            x, y = SPECIAL_COORDS[ch]\n",
        "            outline = plt.Rectangle(\n",
        "                (x - key_width/2, y - key_height/2),\n",
        "                key_width,\n",
        "                key_height,\n",
        "                fill=False,\n",
        "                edgecolor=color,\n",
        "                linewidth=lw,\n",
        "                zorder=4\n",
        "            )\n",
        "            ax.add_patch(outline)\n",
        "\n",
        "    outline_anchor(anchor_char)\n",
        "\n",
        "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "    sm.set_array([])\n",
        "    cbar = fig.colorbar(sm, ax=ax, fraction=0.046, pad=0.04)\n",
        "    cbar.set_label('Predicted count')\n",
        "\n",
        "    ax.set_aspect('equal', 'box')\n",
        "    ax.set_xlim(-1.0, 13.0)\n",
        "    ax.set_ylim(-1, 5.0)\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_title(f'GIK Keyboard Heatmap Prediction when True Key = \"{anchor_char}\", {subset_name} set',\n",
        "                 fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Set Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm_orig = compute_confusion_matrix_40x40(trainer.test_dataset, model, DEVICE)\n",
        "\n",
        "#plot_confusion_matrix_40x40(cm_orig, 'Test')\n",
        "\n",
        "plot_virtual_keyboard_heatmap(cm_orig, 'k', 'Test')\n",
        "\n",
        "# neighbours_a, cm_a = plot_anchor_with_closest_neighbours(cm_orig, 'i', 'Test', k_neighbours=5)\n",
        "\n",
        "# neighbours_g, cm_g = plot_anchor_with_closest_neighbours(cm_orig, 'l', 'Test', k_neighbours=5)\n",
        "\n",
        "# neighbours_l, cm_l = plot_anchor_with_closest_neighbours(cm_orig, 'l', 'Test', k_neighbours=13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm_orig_val = compute_confusion_matrix_40x40(trainer.val_dataset, model, DEVICE)\n",
        "\n",
        "#plot_confusion_matrix_40x40(cm_orig, 'Test')\n",
        "\n",
        "plot_virtual_keyboard_heatmap(cm_orig_val, 'o', 'test')\n",
        "\n",
        "# neighbours_a_val, cm_a_val = plot_anchor_with_closest_neighbours(cm_orig_val, 'd', 'Validation', k_neighbours=12)\n",
        "\n",
        "# neighbours_g_val, cm_g_val = plot_anchor_with_closest_neighbours(cm_orig_val, 'g', 'Validation', k_neighbours=20)\n",
        "\n",
        "# neighbours_l_val, cm_l_val = plot_anchor_with_closest_neighbours(cm_orig_val, 'l', 'Validation', k_neighbours=13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODEL_PATH = os.path.join(DATA_DIR, \"gik_model.pt\")\n",
        "# torch.save({\n",
        "#     'model_state_dict': model.state_dict(),\n",
        "#     'config': CONFIG,\n",
        "#     'input_dim': dataset.input_dim,\n",
        "#     'metadata': metadata\n",
        "# }, MODEL_PATH)\n",
        "# print(f\"Model saved to {MODEL_PATH}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
