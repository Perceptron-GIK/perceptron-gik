{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GIK Character Prediction Model\n",
        "\n",
        "Train a model to predict keyboard characters from IMU sensor data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
        "\n",
        "import sys\n",
        "import yaml\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add project root to path\n",
        "PROJECT_ROOT = os.path.dirname(os.path.abspath('__file__'))\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "from pretraining import preprocess_multiple_sources, load_preprocessed_dataset, export_dataset_to_csv, get_class_weights\n",
        "from src.Constants.char_to_key import INDEX_TO_CHAR, CHAR_TO_INDEX, NUM_CLASSES\n",
        "from src.pre_processing.reduce_dim import reduce_dim\n",
        "from src.visualisation.visualisation import (\n",
        "    compute_confusion_matrix_40x40,\n",
        "    plot_confusion_matrix_40x40,\n",
        "    plot_anchor_with_closest_neighbours,\n",
        "    plot_virtual_keyboard_heatmap,\n",
        "    show_predictions as viz_show_predictions,\n",
        "    show_predictions_coordinate as viz_show_predictions_coordinate,\n",
        ")\n",
        "from ml.models.gik_model import create_model_auto_input_dim, GIKTrainer, decode_predictions\n",
        "from ml.models.loss_functions.custom_losses import FocalLoss, CoordinateLoss, CoordinateLossClassification\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "print(f\"PyTorch {torch.__version__} | Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.Constants.char_to_key import (\n",
        "    KEY_COORDS,\n",
        "    SPECIAL_COORDS,\n",
        "    SPACE_ANCHORS,\n",
        "    ALL_CHARS,\n",
        "    FULL_COORDS,\n",
        ")\n",
        "\n",
        "print(f\"Loaded keyboard coordinate constants for {len(ALL_CHARS)} classes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load experiment config from project-root YAML\n",
        "CONFIG_PATH = os.path.join(PROJECT_ROOT, \"train_config.yaml\")\n",
        "with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    config_data = yaml.safe_load(f)\n",
        "\n",
        "# Data paths (paired keyboard + IMU files)\n",
        "DATA_DIR = config_data[\"data\"][\"data_dir\"]\n",
        "KEYBOARD_FILES = config_data[\"data\"][\"keyboard_files\"]\n",
        "LEFT_FILES = config_data[\"data\"].get(\"left_files\")\n",
        "RIGHT_FILES = config_data[\"data\"].get(\"right_files\")\n",
        "PROCESSED_DATA_PATH = os.path.join(DATA_DIR, \"processed_dataset.pt\")\n",
        "\n",
        "# Build CONFIG from shared + mode-specific sections\n",
        "EXPERIMENT = config_data[\"experiment\"]\n",
        "MODE = EXPERIMENT[\"mode\"]\n",
        "MODE_CONFIG = config_data[\"modes\"][MODE]\n",
        "\n",
        "CONFIG = {\n",
        "    \"max_seq_length\": EXPERIMENT[\"max_seq_length\"],\n",
        "    \"reduce_dim\": EXPERIMENT[\"use_dim_reduction\"],\n",
        "    \"enable_class_weights\": EXPERIMENT[\"use_class_weights\"],\n",
        "    \"run_preprocess\": EXPERIMENT[\"run_preprocess\"],\n",
        "    \"export_dataset_csv\": EXPERIMENT[\"export_dataset_csv\"],\n",
        "    **config_data[\"model\"],\n",
        "    **config_data[\"train\"],\n",
        "    **MODE_CONFIG,\n",
        "}\n",
        "\n",
        "# Resolve object references encoded as strings in YAML\n",
        "KEY_MAPPING_REGISTRY = {\n",
        "    \"FULL_COORDS\": FULL_COORDS,\n",
        "    \"CHAR_TO_INDEX\": CHAR_TO_INDEX,\n",
        "}\n",
        "LOSS_REGISTRY = {\n",
        "    \"CoordinateLossClassification\": CoordinateLossClassification,\n",
        "    \"CoordinateLoss\": CoordinateLoss,\n",
        "    \"FocalLoss\": FocalLoss,\n",
        "}\n",
        "OUTPUT_LOGITS_REGISTRY = {\n",
        "    \"NUM_CLASSES\": NUM_CLASSES,\n",
        "}\n",
        "\n",
        "CONFIG[\"key_mapping_dict\"] = KEY_MAPPING_REGISTRY[CONFIG[\"key_mapping_dict\"]]\n",
        "CONFIG[\"loss\"] = LOSS_REGISTRY[CONFIG[\"loss\"]]\n",
        "if isinstance(CONFIG[\"output_logits\"], str):\n",
        "    CONFIG[\"output_logits\"] = OUTPUT_LOGITS_REGISTRY[CONFIG[\"output_logits\"]]\n",
        "\n",
        "print(f\"Loaded config: {CONFIG_PATH}\")\n",
        "print(f\"Mode: {MODE}\")\n",
        "print(f\"Data dir: {DATA_DIR}\")\n",
        "print(f\"Keyboard files: {KEYBOARD_FILES}\")\n",
        "print(f\"Left IMU files: {LEFT_FILES}\")\n",
        "print(f\"Right IMU files: {RIGHT_FILES}\")\n",
        "print(f\"Model: {CONFIG['model_type']}\")\n",
        "print(f\"Seq length: {CONFIG['max_seq_length']}\")\n",
        "print(f\"Loss: {CONFIG['loss'].__name__}\")\n",
        "print(f\"Run preprocess: {CONFIG['run_preprocess']}\")\n",
        "print(f\"Export CSV: {CONFIG['export_dataset_csv']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if CONFIG[\"run_preprocess\"]:\n",
        "    # Preprocess and combine multiple data sources\n",
        "    metadata = preprocess_multiple_sources(\n",
        "        data_dir=DATA_DIR,\n",
        "        keyboard_files=KEYBOARD_FILES,\n",
        "        left_files=LEFT_FILES,\n",
        "        right_files=RIGHT_FILES,\n",
        "        output_path=PROCESSED_DATA_PATH,\n",
        "        max_seq_length=CONFIG['max_seq_length'],\n",
        "        normalize=True,\n",
        "        apply_filtering=True\n",
        "    )\n",
        "else:\n",
        "    # Assume preprocessing already done and .pt file exists\n",
        "    preprocessed = torch.load(PROCESSED_DATA_PATH, weights_only=False)\n",
        "    metadata = preprocessed[\"metadata\"]\n",
        "    print(f\"Using existing preprocessed dataset: {PROCESSED_DATA_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nTotal Samples: {metadata['num_samples']} | Feat dim: {metadata['feat_dim']} | Sources: {metadata['num_sources']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if CONFIG[\"run_preprocess\"] and CONFIG[\"export_dataset_csv\"]:\n",
        "    # Export to CSV for inspection (optional)\n",
        "    export_dataset_to_csv(PROCESSED_DATA_PATH, DATA_DIR, include_features=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if CONFIG[\"reduce_dim\"]:\n",
        "    DIM_RED_OUTPUT = os.path.join(DATA_DIR, \"dim_red_output.pt\")\n",
        "\n",
        "    # Ideally pass the metadata from preprocessing instead of manually setting it here but this avoids having to rerun the preprocessing step\n",
        "    HAS_LEFT = False\n",
        "    HAS_RIGHT = True\n",
        "\n",
        "    # dims = reduce_dim(\n",
        "    #     data_source=PROCESSED_DATA_PATH,\n",
        "    #     method=\"active-imu\",\n",
        "    #     has_left=HAS_LEFT,\n",
        "    #     has_right=HAS_RIGHT, \n",
        "    #     normalize=True,\n",
        "    #     output_path=DIM_RED_OUTPUT)\n",
        "    \n",
        "    dims = reduce_dim(\n",
        "        data_source=PROCESSED_DATA_PATH,\n",
        "        method=\"pca\",\n",
        "        dims_ratio=0.4,\n",
        "        has_left=HAS_LEFT,\n",
        "        has_right=HAS_RIGHT, \n",
        "        normalize=True,\n",
        "        output_path=DIM_RED_OUTPUT,\n",
        "        root_dir=PROJECT_ROOT)\n",
        "\n",
        "    print(f\"Feature dimension reduced from {dims['dim_bef']} to {dims['dim_aft']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Balance DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if CONFIG[\"enable_class_weights\"]:\n",
        "    class_weights = get_class_weights(DIM_RED_OUTPUT if CONFIG[\"reduce_dim\"] else PROCESSED_DATA_PATH)\n",
        "    # class_weights.to(DEVICE)\n",
        "    if CONFIG[\"loss\"] == FocalLoss:\n",
        "        CONFIG[\"loss_params\"][\"alpha\"] = class_weights\n",
        "    elif CONFIG[\"loss\"] == CoordinateLossClassification:\n",
        "        CONFIG[\"loss_params\"][\"class_weights\"] = class_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset & Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = load_preprocessed_dataset(DIM_RED_OUTPUT if CONFIG[\"reduce_dim\"] else PROCESSED_DATA_PATH, \n",
        "                                    is_one_hot_labels=CONFIG[\"is_one_hot\"],\n",
        "                                    char_to_index=CONFIG[\"key_mapping_dict\"],\n",
        "                                    return_class_id=CONFIG[\"return_class_id\"])\n",
        "print(f\"Dataset: {len(dataset)} samples | Input dim: {dataset.input_dim}\")\n",
        "\n",
        "# Create model\n",
        "model = create_model_auto_input_dim(\n",
        "    dataset,\n",
        "    model_type=CONFIG['model_type'],\n",
        "    hidden_dim_inner_model=CONFIG['hidden_dim_inner_model'],\n",
        "    hidden_dim_classification_head=CONFIG['hidden_dim_classification_head'],\n",
        "    no_layers_classification_head=CONFIG['num_layers'],\n",
        "    dropout_inner_layers=CONFIG['dropout'],\n",
        "    inner_model_kwargs=CONFIG['inner_model_prams'],\n",
        "    output_logits = CONFIG['output_logits'],\n",
        ")\n",
        "\n",
        "# Print model architecture\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Model architecture\")\n",
        "print(\"=\" * 60)\n",
        "print(model)\n",
        "print(\"=\" * 60)\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = GIKTrainer(\n",
        "    model=model,\n",
        "    dataset=dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    learning_rate=CONFIG['learning_rate'],\n",
        "    device=DEVICE,\n",
        "    loss=CONFIG.get('loss'),\n",
        "    loss_kwargs=CONFIG.get('loss_params'),\n",
        "    regression=CONFIG.get('regression'),\n",
        ")\n",
        "\n",
        "history = trainer.train(\n",
        "    epochs=CONFIG['epochs'],\n",
        "    early_stopping_patience=CONFIG['early_stopping']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].plot(history['train_loss'], label='Train')\n",
        "axes[0].plot(history['val_loss'], label='Val')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "axes[0].set_title('Loss')\n",
        "\n",
        "\n",
        "axes[1].plot(history['train_acc'], label='Train')\n",
        "axes[1].plot(history['val_acc'], label='Val')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].set_title('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on validation set\n",
        "val_loss, val_acc = trainer.validate()\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")  \n",
        "# print(\"Validation Accuracy: {val_acc:.2%}\")\n",
        "print()\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = trainer.evaluate_test()\n",
        "print(f\"Test Loss: {test_loss:.4f}\") \n",
        "# print(\" Test Accuracy: {test_acc:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show validation predictions\n",
        "if CONFIG[\"regression\"]:\n",
        "    viz_show_predictions_coordinate(trainer.val_dataset, model, DEVICE, 'Validation')\n",
        "else:\n",
        "    viz_show_predictions(trainer.val_dataset, model, DEVICE, 'Validation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show test predictions\n",
        "if CONFIG[\"regression\"]:\n",
        "    viz_show_predictions_coordinate(trainer.test_dataset, model, DEVICE, 'Test')\n",
        "else:\n",
        "    viz_show_predictions(trainer.test_dataset, model, DEVICE, 'Test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Keyboard Heatmap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Set Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coord_dict = FULL_COORDS if CONFIG[\"regression\"] else None\n",
        "cm_orig = compute_confusion_matrix_40x40(trainer.test_dataset, model, DEVICE, coord_dict=coord_dict)\n",
        "\n",
        "plot_virtual_keyboard_heatmap(cm_orig, 'd', 'Test')\n",
        "\n",
        "# Optional matrix view\n",
        "# plot_confusion_matrix_40x40(cm_orig, 'Test')\n",
        "\n",
        "# neighbours_a, cm_a = plot_anchor_with_closest_neighbours(cm_orig, 'i', 'Test', k_neighbours=5)\n",
        "# neighbours_g, cm_g = plot_anchor_with_closest_neighbours(cm_orig, 'l', 'Test', k_neighbours=5)\n",
        "# neighbours_l, cm_l = plot_anchor_with_closest_neighbours(cm_orig, 'l', 'Test', k_neighbours=13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_virtual_keyboard_heatmap(cm_orig, 's', 'Test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm_orig_val = compute_confusion_matrix_40x40(\n",
        "    trainer.val_dataset,\n",
        "    model,\n",
        "    DEVICE,\n",
        "    coord_dict=FULL_COORDS if CONFIG[\"regression\"] else None,\n",
        ")\n",
        "\n",
        "plot_virtual_keyboard_heatmap(cm_orig_val, 'c', 'Validation')\n",
        "\n",
        "# Optional matrix view\n",
        "# plot_confusion_matrix_40x40(cm_orig_val, 'Validation')\n",
        "\n",
        "# neighbours_a_val, cm_a_val = plot_anchor_with_closest_neighbours(cm_orig_val, 'd', 'Validation', k_neighbours=12)\n",
        "# neighbours_g_val, cm_g_val = plot_anchor_with_closest_neighbours(cm_orig_val, 'g', 'Validation', k_neighbours=20)\n",
        "# neighbours_l_val, cm_l_val = plot_anchor_with_closest_neighbours(cm_orig_val, 'l', 'Validation', k_neighbours=13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODEL_PATH = os.path.join(DATA_DIR, \"gik_model.pt\")\n",
        "# torch.save({\n",
        "#     'model_state_dict': model.state_dict(),\n",
        "#     'config': CONFIG,\n",
        "#     'input_dim': dataset.input_dim,\n",
        "#     'metadata': metadata\n",
        "# }, MODEL_PATH)\n",
        "# print(f\"Model saved to {MODEL_PATH}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
